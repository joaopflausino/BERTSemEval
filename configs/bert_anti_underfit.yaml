experiment:
  name: "bert_anti_underfit"
  description: "BERT optimized to combat underfitting - for when both losses are high"
  seed: 42

model:
  type: "bert"
  name: "bert-base-uncased"
  num_labels: 3
  dropout_prob: 0.05  # Very low dropout to allow learning
  freeze_layers: 0    # Don't freeze any layers

data:
  train_path: "dataset/processed/bert_train.csv"
  eval_path: "dataset/processed/bert_validation.csv"
  max_length: 128

training:
  batch_size: 32  # Larger batch for more stable gradients
  learning_rate: 5e-5  # Higher LR to learn faster
  weight_decay: 0.001  # Minimal weight decay
  num_epochs: 20  # More epochs to learn
  warmup_proportion: 0.2  # Longer warmup
  max_grad_norm: 1.0

  # Use class weights if imbalanced
  use_class_weights: true
  class_weight_method: "balanced"

  # Simple loss for underfitting
  loss_type: "cross_entropy"

  # More lenient early stopping
  early_stopping:
    patience: 5
    min_delta: 0.0005

  # No augmentation to avoid making learning harder
  use_augmentation: false

output:
  output_dir: "experiments/bert_anti_underfit"
  save_best_only: true
