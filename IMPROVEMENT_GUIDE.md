# Guia de Melhorias para Validation Loss Alta

Este guia fornece solu√ß√µes pr√°ticas para melhorar os resultados do modelo quando a validation loss est√° alta.

## üîç Passo 1: Diagnosticar o Problema

Primeiro, identifique exatamente qual √© o problema:

```bash
# Se voc√™ j√° treinou um modelo
python diagnose_model.py --experiment experiments/bert_base

# Se voc√™ quer analisar seus dados
python diagnose_model.py --train-data dataset/processed/bert_train.csv --val-data dataset/processed/bert_validation.csv

# Ambos
python diagnose_model.py --experiment experiments/bert_base --train-data dataset/processed/bert_train.csv
```

O script ir√° diagnosticar se voc√™ tem:
- **OVERFITTING**: Training loss baixa, validation loss alta
- **UNDERFITTING**: Ambas as losses altas
- **DESBALANCEAMENTO**: Classes desbalanceadas nos dados

## üìä Cen√°rio 1: OVERFITTING

**Sintomas:**
- Training loss: 0.2-0.4
- Validation loss: > 0.8
- Gap entre training e validation > 0.4

**Solu√ß√£o: Use a configura√ß√£o anti-overfitting**

```bash
python train_model.py --config configs/bert_anti_overfit.yaml
```

**O que essa config faz:**
- ‚úÖ **Dropout alto (0.4)**: Previne memoriza√ß√£o
- ‚úÖ **Weight decay forte (0.1)**: Penaliza pesos grandes
- ‚úÖ **Learning rate baixo (1e-5)**: Aprendizado mais cauteloso
- ‚úÖ **Freeze layers (6)**: Congela camadas iniciais do BERT
- ‚úÖ **Focal Loss**: Foca em exemplos dif√≠ceis
- ‚úÖ **Early stopping (patience=3)**: Para quando come√ßar a overfitar
- ‚úÖ **Data augmentation (30%)**: Aumenta diversidade dos dados

**Ajustes adicionais se ainda n√£o melhorar:**

1. **Congele mais camadas**:
   ```yaml
   model:
     freeze_layers: 9  # Congela quase todo o BERT
   ```

2. **Aumente dropout ainda mais**:
   ```yaml
   model:
     dropout_prob: 0.5  # M√°ximo recomendado
   ```

3. **Reduza learning rate**:
   ```yaml
   training:
     learning_rate: 5e-6  # Extremamente baixo
   ```

## üìä Cen√°rio 2: UNDERFITTING

**Sintomas:**
- Training loss: > 0.8
- Validation loss: > 0.9
- Modelo n√£o est√° aprendendo adequadamente

**Solu√ß√£o: Use a configura√ß√£o anti-underfitting**

```bash
python train_model.py --config configs/bert_anti_underfit.yaml
```

**O que essa config faz:**
- ‚úÖ **Dropout baixo (0.05)**: Permite que o modelo aprenda mais
- ‚úÖ **Sem freeze**: Todas as camadas treinadas
- ‚úÖ **Learning rate alto (5e-5)**: Aprende mais r√°pido
- ‚úÖ **Batch size maior (32)**: Gradientes mais est√°veis
- ‚úÖ **Mais √©pocas (20)**: Mais tempo para aprender
- ‚úÖ **Weight decay m√≠nimo (0.001)**: Menos penaliza√ß√£o

**Ajustes adicionais:**

1. **Verifique seus dados**:
   ```bash
   python diagnose_model.py --train-data dataset/processed/bert_train.csv
   ```
   - Textos muito curtos ou muito longos?
   - Dados mal preprocessados?
   - Ru√≠do excessivo?

2. **Aumente max_length se textos s√£o longos**:
   ```yaml
   data:
     max_length: 256  # Ou at√© 512
   ```

3. **Experimente learning rate ainda maior**:
   ```yaml
   training:
     learning_rate: 8e-5  # Limite superior recomendado
   ```

## üìä Cen√°rio 3: Configura√ß√£o Balanceada

**Se voc√™ n√£o sabe qual √© o problema ou quer um ponto de partida s√≥lido:**

```bash
python train_model.py --config configs/bert_balanced.yaml
```

**O que essa config faz:**
- ‚úÖ **Par√¢metros balanceados** seguindo best practices
- ‚úÖ **Label smoothing**: Previne overconfidence
- ‚úÖ **Freeze moderado (3 camadas)**: Equil√≠brio entre flexibilidade e overfitting
- ‚úÖ **Augmentation leve (20%)**: Melhora generaliza√ß√£o sem dificultar treino

## üîß Melhorias Avan√ßadas Dispon√≠veis

### 1. Loss Functions Avan√ßadas

Voc√™ pode modificar o tipo de loss no config:

```yaml
training:
  # Focal Loss - melhor para classes desbalanceadas
  loss_type: "focal"
  focal_gamma: 2.0  # Maior = mais foco em exemplos dif√≠ceis

  # Label Smoothing - previne overconfidence
  loss_type: "label_smoothing"
  label_smoothing: 0.1  # 0.1 a 0.2 √© recomendado

  # Symmetric Cross Entropy - robusto a ru√≠do nos labels
  loss_type: "symmetric"

  # Combined - usa m√∫ltiplas losses
  loss_type: "combined"
```

### 2. Layer Freezing

Congele camadas do BERT para reduzir overfitting:

```yaml
model:
  freeze_layers: 6  # Congela primeiras 6 de 12 camadas
  # 0 = nenhuma
  # 6 = metade
  # 9 = quase todas
  # null = apenas embeddings
```

### 3. Data Augmentation

Ative augmentation para melhorar generaliza√ß√£o:

```yaml
training:
  use_augmentation: true
  augmentation_prob: 0.3  # 30% dos dados ser√£o aumentados
```

T√©cnicas aplicadas:
- Synonym replacement
- Random insertion
- Random swap
- Random deletion

### 4. Class Weights

Para dados desbalanceados:

```yaml
training:
  use_class_weights: true
  class_weight_method: "balanced"  # Ou "inverse_freq"
```

### 5. Early Stopping

Ajuste para parar no momento certo:

```yaml
training:
  early_stopping:
    patience: 3      # N√∫mero de √©pocas sem melhoria
    min_delta: 0.001 # Melhoria m√≠nima considerada
```

## üìà Workflow Recomendado

### Passo a Passo:

1. **Diagn√≥stico inicial**:
   ```bash
   python diagnose_model.py --experiment experiments/bert_base --train-data dataset/processed/bert_train.csv
   ```

2. **Identifique o problema** baseado na sa√≠da do diagn√≥stico

3. **Escolha a configura√ß√£o apropriada**:
   - Overfitting ‚Üí `bert_anti_overfit.yaml`
   - Underfitting ‚Üí `bert_anti_underfit.yaml`
   - Incerto ‚Üí `bert_balanced.yaml`

4. **Treine o modelo**:
   ```bash
   python train_model.py --config configs/bert_anti_overfit.yaml
   ```

5. **Analise os resultados**:
   ```bash
   python diagnose_model.py --experiment experiments/bert_anti_overfit
   ```

6. **Itere**: Ajuste hiperpar√¢metros baseado nos resultados

## üéØ Tabela de Refer√™ncia R√°pida

| Problema | Dropout | LR | Weight Decay | Freeze | Epochs | Loss Type |
|----------|---------|-------|--------------|--------|--------|-----------|
| **Overfitting severo** | 0.4-0.5 | 1e-5 | 0.05-0.1 | 6-9 | 10-15 | Focal |
| **Overfitting moderado** | 0.2-0.3 | 2e-5 | 0.02-0.05 | 3-6 | 10-12 | Label Smoothing |
| **Balanceado** | 0.15-0.2 | 2e-5 | 0.01 | 3 | 10-15 | Label Smoothing |
| **Underfitting** | 0.05-0.1 | 3e-5-5e-5 | 0.001-0.01 | 0 | 15-20 | Cross Entropy |

## üí° Dicas Extras

### Se validation loss continua alta:

1. **Verifique preprocessamento**:
   - Remover URLs, men√ß√µes, hashtags
   - Normalizar texto
   - Remover caracteres repetidos

2. **Analise distribui√ß√£o de dados**:
   ```python
   # Verifique se train e validation t√™m distribui√ß√£o similar
   python diagnose_model.py --train-data dataset/processed/bert_train.csv --val-data dataset/processed/bert_validation.csv
   ```

3. **Aumente max_length se textos s√£o longos**:
   ```yaml
   data:
     max_length: 256  # Padr√£o √© 128
   ```

4. **Use modelo maior se dataset √© grande**:
   ```yaml
   model:
     name: "bert-large-uncased"  # Em vez de bert-base
   ```

5. **Combine m√∫ltiplas t√©cnicas**:
   - Focal Loss + Data Augmentation + Layer Freezing
   - Label Smoothing + Class Weights + Early Stopping

## üìù Exemplos de Uso Completo

### Exemplo 1: Resolver Overfitting Severo

```bash
# 1. Diagn√≥stico
python diagnose_model.py --experiment experiments/bert_base

# Output mostra: Training Loss: 0.25, Validation Loss: 1.1 (GAP: 0.85)

# 2. Treinar com config anti-overfitting
python train_model.py --config configs/bert_anti_overfit.yaml

# 3. Verificar melhoria
python diagnose_model.py --experiment experiments/bert_anti_overfit

# Output esperado: Gap reduzido para ~0.3-0.4
```

### Exemplo 2: Melhorar Underfitting

```bash
# 1. Diagn√≥stico mostra ambas losses altas (>0.8)

# 2. Verificar dados
python diagnose_model.py --train-data dataset/processed/bert_train.csv

# 3. Treinar com config anti-underfitting
python train_model.py --config configs/bert_anti_underfit.yaml

# 4. Se ainda n√£o melhorar, aumentar learning rate manualmente
# Editar configs/bert_anti_underfit.yaml e trocar learning_rate para 8e-5
```

## üöÄ Melhores Pr√°ticas

1. **Sempre comece com diagn√≥stico**
2. **Mude um hiperpar√¢metro por vez** para entender o impacto
3. **Use early stopping** para evitar desperd√≠cio de tempo
4. **Salve m√∫ltiplos experimentos** e compare com `compare_models.py`
5. **Documente o que funciona** para seu dataset espec√≠fico

## üìû Troubleshooting

### "Training loss n√£o diminui"
- Aumente learning rate (3e-5 a 5e-5)
- Reduza weight decay (0.001)
- Verifique se dados est√£o carregados corretamente

### "Validation loss oscila muito"
- Reduza learning rate
- Aumente batch size
- Use warmup maior (0.15-0.2)

### "Modelo converge muito r√°pido e overfita"
- Use early stopping com patience=3
- Aumente dropout
- Ative data augmentation

### "Classes desbalanceadas"
- Use `use_class_weights: true`
- Experimente Focal Loss
- Considere oversampling da classe minorit√°ria

---

**√öltima atualiza√ß√£o**: 2024-10-29
